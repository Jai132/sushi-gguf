{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnTVVn_LLQKn",
        "outputId": "ce8e60fe-29c0-4f6c-f51c-bb459dc02af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing previous version\n",
            "Cloning into 'new-gguf-test'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 13 (delta 4), reused 11 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (13/13), 7.88 KiB | 7.88 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('./sushi-gguf/'):\n",
        "  print('Removing previous version')\n",
        "  !rm -rf sushi-gguf-test/\n",
        "!git clone https://github.com/nerdlab53/sushi-gguf.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGyrpue8ln57",
        "outputId": "a41d3496-99fc-46e1-db95-6aad6d0e5d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[32m╔═══════════════════════════════════════════════════════════════════════════════╗\u001b[0m\n",
            "\u001b[32m║\u001b[0m                                                                               \u001b[32m║\u001b[0m\n",
            "\u001b[32m║\u001b[0m      \u001b[1;34mSDXL Model GGUF Quantization Tool\u001b[0m                                        \u001b[32m║\u001b[0m\n",
            "\u001b[32m║\u001b[0m      \u001b[36mConvert and quantize your Stable Diffusion XL models to GGUF format\u001b[0m      \u001b[32m║\u001b[0m\n",
            "\u001b[32m║\u001b[0m                                                                               \u001b[32m║\u001b[0m\n",
            "\u001b[32m╚═══════════════════════════════════════════════════════════════════════════════╝\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[33mASCII art module not found. Continuing without banner\u001b[0m\u001b[33m...\u001b[0m\n",
            "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────\u001b[0m\u001b[33m \u001b[0m\u001b[1;33mProcess Steps\u001b[0m\u001b[33m \u001b[0m\u001b[33m─────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
            "\u001b[33m│\u001b[0m                                                                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m  \u001b[1;35m \u001b[0m\u001b[1;35m#  \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mStep              \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mDescription                                     \u001b[0m\u001b[1;35m \u001b[0m                   \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m  ─────────────────────────────────────────────────────────────────────────────                   \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m  \u001b[2m \u001b[0m\u001b[2m1  \u001b[0m\u001b[2m \u001b[0m \u001b[36m \u001b[0m\u001b[36mExtract Components\u001b[0m\u001b[36m \u001b[0m \u001b[32m \u001b[0m\u001b[32mExtract UNet, CLIP_L, CLIP_G and VAE from model \u001b[0m\u001b[32m \u001b[0m                   \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m  \u001b[2m \u001b[0m\u001b[2m2  \u001b[0m\u001b[2m \u001b[0m \u001b[36m \u001b[0m\u001b[36mConvert to GGUF   \u001b[0m\u001b[36m \u001b[0m \u001b[32m \u001b[0m\u001b[32mConvert UNet to GGUF format                     \u001b[0m\u001b[32m \u001b[0m                   \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m  \u001b[2m \u001b[0m\u001b[2m3  \u001b[0m\u001b[2m \u001b[0m \u001b[36m \u001b[0m\u001b[36mQuantize          \u001b[0m\u001b[36m \u001b[0m \u001b[32m \u001b[0m\u001b[32mCreate quantized versions with reduced precision\u001b[0m\u001b[32m \u001b[0m                   \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m                                                                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[36m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
            "\u001b[36m│\u001b[0m \u001b[1;36mDownloading model from CivitAI: bluepencil-xl (Version: 592322)\u001b[0m                                  \u001b[36m│\u001b[0m\n",
            "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[?25l\r\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mDownloading bluepencil-xl\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m2025-03-09 16:11:01,719 - INFO - Downloading model from CivitAI: bluepencil-xl.safetensors\n",
            "2025-03-09 16:11:01,719 - INFO - Output path: bluepencil-xl.safetensors\n",
            "--2025-03-09 16:11:01--  https://civitai.com/api/download/models/592322?token=b7b0783ae0858232a5d10cce4a599062\n",
            "Resolving civitai.com (civitai.com)... 104.22.19.237, 172.67.12.143, 104.22.18.237, ...\n",
            "Connecting to civitai.com (civitai.com)|104.22.19.237|:443... connected.\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m \u001b[1;36mDownloading bluepencil-xl\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m307 Temporary Redirect\n",
            "Location: https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/93467/bluePencilXLV700.Uake.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22bluePencilXL_v700.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250309/us-east-1/s3/aws4_request&X-Amz-Date=20250309T161102Z&X-Amz-SignedHeaders=host&X-Amz-Signature=1160c5763831783f9badd584430a9d11e773cdb479f9afbdf44e63215b75de51 [following]\n",
            "--2025-03-09 16:11:02--  https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/93467/bluePencilXLV700.Uake.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22bluePencilXL_v700.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250309/us-east-1/s3/aws4_request&X-Amz-Date=20250309T161102Z&X-Amz-SignedHeaders=host&X-Amz-Signature=1160c5763831783f9badd584430a9d11e773cdb479f9afbdf44e63215b75de51\n",
            "Resolving civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)... 162.159.141.50, 172.66.1.46, 2a06:98c1:58::12e, ...\n",
            "Connecting to civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)|162.159.141.50|:443... connected.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m \u001b[1;36mDownloading bluepencil-xl\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "2025-03-09 16:11:02,536 - INFO - Model downloaded successfully to: bluepencil-xl.safetensors\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mDownloading bluepencil-xl\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mComplete!\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[32m✓ Successfully downloaded model to: \u001b[0m\u001b[1;32mbluepencil-xl.safetensors\u001b[0m\n",
            "\u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────────────\u001b[0m\u001b[33m \u001b[0m\u001b[1;33mModel Information\u001b[0m\u001b[33m \u001b[0m\u001b[33m───────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
            "\u001b[33m│\u001b[0m \u001b[36m \u001b[0m\u001b[36mFile Name\u001b[0m\u001b[36m \u001b[0m\u001b[32m \u001b[0m\u001b[32mbluepencil-xl.safetensors\u001b[0m\u001b[32m \u001b[0m                                                           \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m \u001b[36m \u001b[0m\u001b[36mFile Size\u001b[0m\u001b[36m \u001b[0m\u001b[32m \u001b[0m\u001b[32m6.46 GB                  \u001b[0m\u001b[32m \u001b[0m                                                           \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m \u001b[36m \u001b[0m\u001b[36mLocation \u001b[0m\u001b[36m \u001b[0m\u001b[32m \u001b[0m\u001b[32mbluepencil-xl.safetensors\u001b[0m\u001b[32m \u001b[0m                                                           \u001b[33m│\u001b[0m\n",
            "\u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[36m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
            "\u001b[36m│\u001b[0m \u001b[1;36mSTEP 1: Extracting Model Components\u001b[0m                                                              \u001b[36m│\u001b[0m\n",
            "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mExtracting model components\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m2025-03-09 16:11:02,545 - INFO - Loading checkpoint from: bluepencil-xl.safetensors\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m \u001b[1;36mExtracting model components\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:01\u001b[0m2025-03-09 16:11:03,808 - INFO - Saving UNet weights to: components/bluepencil-xl_unet.safetensors\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m \u001b[1;36mExtracting model components\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:29\u001b[0m2025-03-09 16:14:31,634 - INFO - Saving CLIP_L weights to: components/bluepencil-xl_clip_l.safetensors\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m \u001b[1;36mExtracting model components\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:30\u001b[0m2025-03-09 16:14:41,190 - INFO - Saving CLIP_G weights to: components/bluepencil-xl_clip_g.safetensors\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m \u001b[1;36mExtracting model components\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:49\u001b[0m2025-03-09 16:15:29,009 - INFO - Saving VAE weights to: components/bluepencil-xl_vae.safetensors\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m \u001b[1;36mExtracting model components\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mComplete!\u001b[0m \u001b[33m0:04:30\u001b[0m\n",
            "\u001b[?25h\u001b[32m✓ Components extracted. UNet saved to: \u001b[0m\u001b[1;32mcomponents/bluepencil-xl_unet.safetensors\u001b[0m\n",
            "\u001b[36m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
            "\u001b[36m│\u001b[0m \u001b[1;36mSTEP 2: Converting UNet to GGUF Format\u001b[0m                                                           \u001b[36m│\u001b[0m\n",
            "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m2025-03-09 16:15:33,184 - INFO - Setting up llama.cpp...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m2025-03-09 16:15:33,287 - INFO - Removed existing llama.cpp directory\n",
            "Cloning into 'llama.cpp'...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:01\u001b[0mremote: Enumerating objects: 45734, done.\u001b[K\n",
            "remote: Counting objects: 100% (341/341), done.\u001b[K\n",
            "remote: Compressing objects: 100% (253/253), done.\u001b[K\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:06\u001b[0mremote: Total 45734 (delta 230), reused 88 (delta 88), pack-reused 45393 (from 4)\u001b[K\n",
            "Receiving objects: 100% (45734/45734), 96.68 MiB | 21.44 MiB/s, done.\n",
            "Resolving deltas: 100% (32969/32969), done.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:19\u001b[0m2025-03-09 16:15:53,090 - INFO - Cloned llama.cpp repository\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:19\u001b[0m2025-03-09 16:15:53,198 - INFO - gguf already installed\n",
            "2025-03-09 16:15:53,200 - WARNING - Can't change directory to llama.cpp\n",
            "2025-03-09 16:15:53,200 - INFO - Downloading conversion script and patch...\n",
            "--2025-03-09 16:15:53--  https://raw.githubusercontent.com/city96/ComfyUI-GGUF/main/tools/convert.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:20\u001b[0mconnected.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:20\u001b[0m200 OK\n",
            "Length: 9380 (9.2K) [text/plain]\n",
            "Saving to: ‘convert.py’\n",
            "\n",
            "convert.py          100%[===================>]   9.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-09 16:15:53 (53.5 MB/s) - ‘convert.py’ saved [9380/9380]\n",
            "\n",
            "--2025-03-09 16:15:53--  https://huggingface.co/Old-Fisherman/SDXL_Finetune_GGUF_Files/resolve/main/convert_g.py\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.49, 18.239.50.80, 18.239.50.103, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.49|:443... connected.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:20\u001b[0m200 OK\n",
            "Length: 5077 (5.0K) [text/plain]\n",
            "Saving to: ‘convert_g.py’\n",
            "\n",
            "convert_g.py        100%[===================>]   4.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-09 16:15:53 (1020 MB/s) - ‘convert_g.py’ saved [5077/5077]\n",
            "\n",
            "--2025-03-09 16:15:53--  https://raw.githubusercontent.com/city96/ComfyUI-GGUF/main/tools/lcpp.patch\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:20\u001b[0m200 OK\n",
            "Length: 9856 (9.6K) [text/plain]\n",
            "Saving to: ‘lcpp.patch’\n",
            "\n",
            "lcpp.patch          100%[===================>]   9.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-09 16:15:53 (42.0 MB/s) - ‘lcpp.patch’ saved [9856/9856]\n",
            "\n",
            "2025-03-09 16:15:53,754 - INFO - Applying patch to llama.cpp...\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:20\u001b[0mNote: switching to 'tags/b3600'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 2fb92678 Fix incorrect use of ctx_split for bias tensors (#9063)\n",
            "2025-03-09 16:15:54,019 - INFO - Building llama.cpp...\n",
            "2025-03-09 16:15:54,019 - INFO - Trying CMake build .. \n",
            "\u001b[2K\u001b[32m⠸\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- The C compiler identification is GNU 11.4.0\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- Detecting CXX compiler ABI info - done\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:21\u001b[0m-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:22\u001b[0m-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:22\u001b[0m-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- OpenMP found\n",
            "-- Using llamafile\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:22\u001b[0m-- Configuring done (1.5s)\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:22\u001b[0m-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:22\u001b[0m[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[ 13%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o\u001b[0m\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:23\u001b[0m[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:23\u001b[0m[ 33%] Built target build_info\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:56\u001b[0m[ 40%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:56\u001b[0m[ 40%] Built target ggml\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:30\u001b[0m[ 66%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:31\u001b[0m[ 66%] Built target llama\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:31\u001b[0m[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:03:31\u001b[0m[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:26\u001b[0m[ 93%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:26\u001b[0m[ 93%] Built target common\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:26\u001b[0m[100%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:29\u001b[0m[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:29\u001b[0m[100%] Built target llama-quantize\n",
            "2025-03-09 16:20:02,713 - INFO - llama.cpp setup complete\n",
            "2025-03-09 16:20:02,714 - INFO - Converting UNet to GGUF: components/bluepencil-xl_unet.safetensors\n",
            "2025-03-09 16:20:02,714 - INFO - Running command: python llama.cpp/convert.py --src \"/content/components/bluepencil-xl_unet.safetensors\" --dst \"/content/gguf/bluepencil-xl-F16.gguf\"\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:38\u001b[0m* Architecture detected from input: sdxl\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:18:58\u001b[0mTraceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert.py\", line 242, in <module>\n",
            "    input(\"Output exists enter to continue or ctrl+c to abort!\")\n",
            "KeyboardInterrupt\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:18:58\u001b[0m2025-03-09 16:34:31,729 - WARNING - First conversion attempt failed: Command exited with code 2\n",
            "2025-03-09 16:34:31,729 - INFO - GGUF conversion complete. Output saved to: gguf/bluepencil-xl-F16.gguf\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[1;36mConverting to GGUF format\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mComplete!\u001b[0m \u001b[33m0:18:58\u001b[0m\n",
            "\u001b[?25h\u001b[32m✓ Conversion complete. GGUF saved to: \u001b[0m\u001b[1;32mgguf/bluepencil-xl-F16.gguf\u001b[0m\n",
            "\u001b[36m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
            "\u001b[36m│\u001b[0m \u001b[1;36mSTEP 3: Quantizing GGUF Model\u001b[0m                                                                    \u001b[36m│\u001b[0m\n",
            "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m \u001b[1;36mQuantizing to Q5_K_S\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:00:00\u001b[0m2025-03-09 16:34:31,737 - INFO - Running quantization: ./build/bin/llama-quantize /content/gguf/bluepencil-xl-F16.gguf /content/quantized/bluepencil-xl_Q5_K_S.gguf Q5_K_S\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m \u001b[1;36mQuantizing to Q5_K_S\u001b[0m \u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:59\u001b[0m2025-03-09 16:39:30,844 - INFO - Quantization output: \n",
            "main: quantize time = 299081.29 ms\n",
            "main:    total time = 299081.29 ms\n",
            "\n",
            "2025-03-09 16:39:30,845 - WARNING - Quantization stderr: main: build = 3600 (2fb92678)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/gguf/bluepencil-xl-F16.gguf' to '/content/quantized/bluepencil-xl_Q5_K_S.gguf' as Q5_K_S\n",
            "llama_model_loader: loaded meta data with 135 key-value pairs and 1680 tensors from /content/gguf/bluepencil-xl-F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = sdxl\n",
            "llama_model_loader: - kv   1:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv   2:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv   3: comfy.gguf.orig_shape.input_blocks.0.0.weight arr[i32,4]       = [320, 4, 3, 3]\n",
            "llama_model_loader: - kv   4: comfy.gguf.orig_shape.input_blocks.1.0.in_layers.2.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv   5: comfy.gguf.orig_shape.input_blocks.1.0.out_layers.3.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv   6: comfy.gguf.orig_shape.input_blocks.2.0.in_layers.2.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv   7: comfy.gguf.orig_shape.input_blocks.2.0.out_layers.3.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv   8: comfy.gguf.orig_shape.input_blocks.3.0.op.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv   9: comfy.gguf.orig_shape.input_blocks.4.0.in_layers.2.weight arr[i32,4]       = [640, 320, 3, 3]\n",
            "llama_model_loader: - kv  10: comfy.gguf.orig_shape.input_blocks.4.0.out_layers.3.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv  11: comfy.gguf.orig_shape.input_blocks.4.0.skip_connection.weight arr[i32,4]       = [640, 320, 1, 1]\n",
            "llama_model_loader: - kv  12: comfy.gguf.orig_shape.input_blocks.4.1.proj_in.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  13: comfy.gguf.orig_shape.input_blocks.4.1.proj_out.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  14: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  15: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  16: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  17: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  18: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  19: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  20: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  21: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  22: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  23: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  24: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  25: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  26: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  27: comfy.gguf.orig_shape.input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  28: comfy.gguf.orig_shape.input_blocks.5.0.in_layers.2.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv  29: comfy.gguf.orig_shape.input_blocks.5.0.out_layers.3.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv  30: comfy.gguf.orig_shape.input_blocks.5.1.proj_in.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  31: comfy.gguf.orig_shape.input_blocks.5.1.proj_out.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  32: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  33: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  34: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  35: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  36: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  37: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  38: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  39: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  40: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  41: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  42: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  43: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  44: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  45: comfy.gguf.orig_shape.input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  46: comfy.gguf.orig_shape.input_blocks.6.0.op.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv  47: comfy.gguf.orig_shape.input_blocks.7.0.in_layers.2.weight arr[i32,4]       = [1280, 640, 3, 3]\n",
            "llama_model_loader: - kv  48: comfy.gguf.orig_shape.input_blocks.7.0.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  49: comfy.gguf.orig_shape.input_blocks.7.0.skip_connection.weight arr[i32,4]       = [1280, 640, 1, 1]\n",
            "llama_model_loader: - kv  50: comfy.gguf.orig_shape.input_blocks.8.0.in_layers.2.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  51: comfy.gguf.orig_shape.input_blocks.8.0.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  52: comfy.gguf.orig_shape.middle_block.0.in_layers.2.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  53: comfy.gguf.orig_shape.middle_block.0.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  54: comfy.gguf.orig_shape.middle_block.2.in_layers.2.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  55: comfy.gguf.orig_shape.middle_block.2.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  56:         comfy.gguf.orig_shape.out.2.weight arr[i32,4]       = [4, 320, 3, 3]\n",
            "llama_model_loader: - kv  57: comfy.gguf.orig_shape.output_blocks.0.0.in_layers.2.weight arr[i32,4]       = [1280, 2560, 3, 3]\n",
            "llama_model_loader: - kv  58: comfy.gguf.orig_shape.output_blocks.0.0.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  59: comfy.gguf.orig_shape.output_blocks.0.0.skip_connection.weight arr[i32,4]       = [1280, 2560, 1, 1]\n",
            "llama_model_loader: - kv  60: comfy.gguf.orig_shape.output_blocks.1.0.in_layers.2.weight arr[i32,4]       = [1280, 2560, 3, 3]\n",
            "llama_model_loader: - kv  61: comfy.gguf.orig_shape.output_blocks.1.0.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  62: comfy.gguf.orig_shape.output_blocks.1.0.skip_connection.weight arr[i32,4]       = [1280, 2560, 1, 1]\n",
            "llama_model_loader: - kv  63: comfy.gguf.orig_shape.output_blocks.2.0.in_layers.2.weight arr[i32,4]       = [1280, 1920, 3, 3]\n",
            "llama_model_loader: - kv  64: comfy.gguf.orig_shape.output_blocks.2.0.out_layers.3.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  65: comfy.gguf.orig_shape.output_blocks.2.0.skip_connection.weight arr[i32,4]       = [1280, 1920, 1, 1]\n",
            "llama_model_loader: - kv  66: comfy.gguf.orig_shape.output_blocks.2.2.conv.weight arr[i32,4]       = [1280, 1280, 3, 3]\n",
            "llama_model_loader: - kv  67: comfy.gguf.orig_shape.output_blocks.3.0.in_layers.2.weight arr[i32,4]       = [640, 1920, 3, 3]\n",
            "llama_model_loader: - kv  68: comfy.gguf.orig_shape.output_blocks.3.0.out_layers.3.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv  69: comfy.gguf.orig_shape.output_blocks.3.0.skip_connection.weight arr[i32,4]       = [640, 1920, 1, 1]\n",
            "llama_model_loader: - kv  70: comfy.gguf.orig_shape.output_blocks.3.1.proj_in.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  71: comfy.gguf.orig_shape.output_blocks.3.1.proj_out.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  72: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  73: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  74: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  75: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  76: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  77: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  78: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  79: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  80: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  81: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  82: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  83: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  84: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  85: comfy.gguf.orig_shape.output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  86: comfy.gguf.orig_shape.output_blocks.4.0.in_layers.2.weight arr[i32,4]       = [640, 1280, 3, 3]\n",
            "llama_model_loader: - kv  87: comfy.gguf.orig_shape.output_blocks.4.0.out_layers.3.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv  88: comfy.gguf.orig_shape.output_blocks.4.0.skip_connection.weight arr[i32,4]       = [640, 1280, 1, 1]\n",
            "llama_model_loader: - kv  89: comfy.gguf.orig_shape.output_blocks.4.1.proj_in.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  90: comfy.gguf.orig_shape.output_blocks.4.1.proj_out.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  91: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  92: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  93: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  94: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  95: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  96: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  97: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv  98: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv  99: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 100: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 101: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 102: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 103: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 104: comfy.gguf.orig_shape.output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv 105: comfy.gguf.orig_shape.output_blocks.5.0.in_layers.2.weight arr[i32,4]       = [640, 960, 3, 3]\n",
            "llama_model_loader: - kv 106: comfy.gguf.orig_shape.output_blocks.5.0.out_layers.3.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv 107: comfy.gguf.orig_shape.output_blocks.5.0.skip_connection.weight arr[i32,4]       = [640, 960, 1, 1]\n",
            "llama_model_loader: - kv 108: comfy.gguf.orig_shape.output_blocks.5.1.proj_in.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 109: comfy.gguf.orig_shape.output_blocks.5.1.proj_out.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 110: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 111: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 112: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 113: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 114: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 115: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 116: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv 117: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.attn1.to_k.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 118: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 119: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.attn1.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 120: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.attn1.to_v.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 121: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 122: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.attn2.to_q.weight arr[i32,2]       = [640, 640]\n",
            "llama_model_loader: - kv 123: comfy.gguf.orig_shape.output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight arr[i32,2]       = [5120, 640]\n",
            "llama_model_loader: - kv 124: comfy.gguf.orig_shape.output_blocks.5.2.conv.weight arr[i32,4]       = [640, 640, 3, 3]\n",
            "llama_model_loader: - kv 125: comfy.gguf.orig_shape.output_blocks.6.0.in_layers.2.weight arr[i32,4]       = [320, 960, 3, 3]\n",
            "llama_model_loader: - kv 126: comfy.gguf.orig_shape.output_blocks.6.0.out_layers.3.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv 127: comfy.gguf.orig_shape.output_blocks.6.0.skip_connection.weight arr[i32,4]       = [320, 960, 1, 1]\n",
            "llama_model_loader: - kv 128: comfy.gguf.orig_shape.output_blocks.7.0.in_layers.2.weight arr[i32,4]       = [320, 640, 3, 3]\n",
            "llama_model_loader: - kv 129: comfy.gguf.orig_shape.output_blocks.7.0.out_layers.3.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv 130: comfy.gguf.orig_shape.output_blocks.7.0.skip_connection.weight arr[i32,4]       = [320, 640, 1, 1]\n",
            "llama_model_loader: - kv 131: comfy.gguf.orig_shape.output_blocks.8.0.in_layers.2.weight arr[i32,4]       = [320, 640, 3, 3]\n",
            "llama_model_loader: - kv 132: comfy.gguf.orig_shape.output_blocks.8.0.out_layers.3.weight arr[i32,4]       = [320, 320, 3, 3]\n",
            "llama_model_loader: - kv 133: comfy.gguf.orig_shape.output_blocks.8.0.skip_connection.weight arr[i32,4]       = [320, 640, 1, 1]\n",
            "llama_model_loader: - kv 134:  comfy.gguf.orig_shape.time_embed.0.weight arr[i32,2]       = [1280, 320]\n",
            "llama_model_loader: - type  f16: 1680 tensors\n",
            "[   1/1680]                input_blocks.0.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[   2/1680]              input_blocks.0.0.weight - [  256,    45,     1,     1], type =    f16, converting to f32 .. size =     0.02 MiB ->     0.04 MiB\n",
            "[   3/1680]   input_blocks.1.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[   4/1680] input_blocks.1.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[   5/1680]    input_blocks.1.0.in_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[   6/1680]  input_blocks.1.0.in_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[   7/1680]    input_blocks.1.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[   8/1680]  input_blocks.1.0.in_layers.2.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[   9/1680]   input_blocks.1.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  10/1680] input_blocks.1.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  11/1680]   input_blocks.1.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  12/1680] input_blocks.1.0.out_layers.3.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[  13/1680]   input_blocks.2.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  14/1680] input_blocks.2.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  15/1680]    input_blocks.2.0.in_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  16/1680]  input_blocks.2.0.in_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  17/1680]    input_blocks.2.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  18/1680]  input_blocks.2.0.in_layers.2.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[  19/1680]   input_blocks.2.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  20/1680] input_blocks.2.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  21/1680]   input_blocks.2.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  22/1680] input_blocks.2.0.out_layers.3.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[  23/1680]             input_blocks.3.0.op.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  24/1680]           input_blocks.3.0.op.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[  25/1680]   input_blocks.4.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  26/1680] input_blocks.4.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[  27/1680]    input_blocks.4.0.in_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  28/1680]  input_blocks.4.0.in_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  29/1680]    input_blocks.4.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  30/1680]  input_blocks.4.0.in_layers.2.weight - [  256,  7200,     1,     1], type =    f16, converting to q5_K .. size =     3.52 MiB ->     1.21 MiB\n",
            "[  31/1680]   input_blocks.4.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  32/1680] input_blocks.4.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  33/1680]   input_blocks.4.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  34/1680] input_blocks.4.0.out_layers.3.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[  35/1680] input_blocks.4.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  36/1680] input_blocks.4.0.skip_connection.weight - [  256,   800,     1,     1], type =    f16, converting to q5_K .. size =     0.39 MiB ->     0.13 MiB\n",
            "[  37/1680]           input_blocks.4.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  38/1680]         input_blocks.4.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  39/1680]        input_blocks.4.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  40/1680]      input_blocks.4.1.proj_in.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[  41/1680]       input_blocks.4.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  42/1680]     input_blocks.4.1.proj_out.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[  43/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  44/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  45/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  46/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  47/1680] input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  48/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[  49/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  50/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  51/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  52/1680] input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[  53/1680] input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[  54/1680] input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[  55/1680] input_blocks.4.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  56/1680] input_blocks.4.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[  57/1680] input_blocks.4.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  58/1680] input_blocks.4.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  59/1680] input_blocks.4.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  60/1680] input_blocks.4.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  61/1680] input_blocks.4.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  62/1680] input_blocks.4.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  63/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  64/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  65/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  66/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  67/1680] input_blocks.4.1.transformer_blocks.1.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  68/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[  69/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  70/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  71/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[  72/1680] input_blocks.4.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[  73/1680] input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[  74/1680] input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[  75/1680] input_blocks.4.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  76/1680] input_blocks.4.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[  77/1680] input_blocks.4.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  78/1680] input_blocks.4.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  79/1680] input_blocks.4.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  80/1680] input_blocks.4.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  81/1680] input_blocks.4.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  82/1680] input_blocks.4.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  83/1680]   input_blocks.5.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  84/1680] input_blocks.5.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[  85/1680]    input_blocks.5.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  86/1680]  input_blocks.5.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  87/1680]    input_blocks.5.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  88/1680]  input_blocks.5.0.in_layers.2.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[  89/1680]   input_blocks.5.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  90/1680] input_blocks.5.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  91/1680]   input_blocks.5.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  92/1680] input_blocks.5.0.out_layers.3.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[  93/1680]           input_blocks.5.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  94/1680]         input_blocks.5.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  95/1680]        input_blocks.5.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  96/1680]      input_blocks.5.1.proj_in.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[  97/1680]       input_blocks.5.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[  98/1680]     input_blocks.5.1.proj_out.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[  99/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 100/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 101/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 102/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 103/1680] input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 104/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[ 105/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 106/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 107/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 108/1680] input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[ 109/1680] input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[ 110/1680] input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[ 111/1680] input_blocks.5.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 112/1680] input_blocks.5.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 113/1680] input_blocks.5.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 114/1680] input_blocks.5.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 115/1680] input_blocks.5.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 116/1680] input_blocks.5.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 117/1680] input_blocks.5.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 118/1680] input_blocks.5.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 119/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 120/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 121/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 122/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 123/1680] input_blocks.5.1.transformer_blocks.1.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 124/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[ 125/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 126/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 127/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[ 128/1680] input_blocks.5.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[ 129/1680] input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[ 130/1680] input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[ 131/1680] input_blocks.5.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 132/1680] input_blocks.5.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 133/1680] input_blocks.5.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 134/1680] input_blocks.5.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 135/1680] input_blocks.5.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 136/1680] input_blocks.5.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 137/1680] input_blocks.5.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 138/1680] input_blocks.5.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 139/1680]             input_blocks.6.0.op.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 140/1680]           input_blocks.6.0.op.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[ 141/1680]   input_blocks.7.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 142/1680] input_blocks.7.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 143/1680]    input_blocks.7.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 144/1680]  input_blocks.7.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 145/1680]    input_blocks.7.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 146/1680]  input_blocks.7.0.in_layers.2.weight - [  256, 28800,     1,     1], type =    f16, converting to q5_K .. size =    14.06 MiB ->     4.83 MiB\n",
            "[ 147/1680]   input_blocks.7.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 148/1680] input_blocks.7.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 149/1680]   input_blocks.7.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 150/1680] input_blocks.7.0.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 151/1680] input_blocks.7.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 152/1680] input_blocks.7.0.skip_connection.weight - [  256,  3200,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[ 153/1680]           input_blocks.7.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 154/1680]         input_blocks.7.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 155/1680]        input_blocks.7.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 156/1680]      input_blocks.7.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 157/1680]       input_blocks.7.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 158/1680]     input_blocks.7.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 159/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 160/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 161/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 162/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 163/1680] input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 164/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 165/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 166/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 167/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 168/1680] input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 169/1680] input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 170/1680] input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 171/1680] input_blocks.7.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 172/1680] input_blocks.7.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 173/1680] input_blocks.7.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 174/1680] input_blocks.7.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 175/1680] input_blocks.7.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 176/1680] input_blocks.7.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 177/1680] input_blocks.7.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 178/1680] input_blocks.7.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 179/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 180/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 181/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 182/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 183/1680] input_blocks.7.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 184/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 185/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 186/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 187/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 188/1680] input_blocks.7.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 189/1680] input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 190/1680] input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 191/1680] input_blocks.7.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 192/1680] input_blocks.7.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 193/1680] input_blocks.7.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 194/1680] input_blocks.7.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 195/1680] input_blocks.7.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 196/1680] input_blocks.7.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 197/1680] input_blocks.7.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 198/1680] input_blocks.7.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 199/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 200/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 201/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 202/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 203/1680] input_blocks.7.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 204/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 205/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 206/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 207/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 208/1680] input_blocks.7.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 209/1680] input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 210/1680] input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 211/1680] input_blocks.7.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 212/1680] input_blocks.7.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 213/1680] input_blocks.7.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 214/1680] input_blocks.7.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 215/1680] input_blocks.7.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 216/1680] input_blocks.7.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 217/1680] input_blocks.7.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 218/1680] input_blocks.7.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 219/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 220/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 221/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 222/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 223/1680] input_blocks.7.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 224/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 225/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 226/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 227/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 228/1680] input_blocks.7.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 229/1680] input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 230/1680] input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 231/1680] input_blocks.7.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 232/1680] input_blocks.7.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 233/1680] input_blocks.7.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 234/1680] input_blocks.7.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 235/1680] input_blocks.7.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 236/1680] input_blocks.7.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 237/1680] input_blocks.7.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 238/1680] input_blocks.7.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 239/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 240/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 241/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 242/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 243/1680] input_blocks.7.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 244/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 245/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 246/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 247/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 248/1680] input_blocks.7.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 249/1680] input_blocks.7.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 250/1680] input_blocks.7.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 251/1680] input_blocks.7.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 252/1680] input_blocks.7.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 253/1680] input_blocks.7.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 254/1680] input_blocks.7.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 255/1680] input_blocks.7.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 256/1680] input_blocks.7.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 257/1680] input_blocks.7.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 258/1680] input_blocks.7.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 259/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 260/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 261/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 262/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 263/1680] input_blocks.7.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 264/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 265/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 266/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 267/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 268/1680] input_blocks.7.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 269/1680] input_blocks.7.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 270/1680] input_blocks.7.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 271/1680] input_blocks.7.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 272/1680] input_blocks.7.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 273/1680] input_blocks.7.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 274/1680] input_blocks.7.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 275/1680] input_blocks.7.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 276/1680] input_blocks.7.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 277/1680] input_blocks.7.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 278/1680] input_blocks.7.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 279/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 280/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 281/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 282/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 283/1680] input_blocks.7.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 284/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 285/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 286/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 287/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 288/1680] input_blocks.7.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 289/1680] input_blocks.7.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 290/1680] input_blocks.7.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 291/1680] input_blocks.7.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 292/1680] input_blocks.7.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 293/1680] input_blocks.7.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 294/1680] input_blocks.7.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 295/1680] input_blocks.7.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 296/1680] input_blocks.7.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 297/1680] input_blocks.7.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 298/1680] input_blocks.7.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 299/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 300/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 301/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 302/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 303/1680] input_blocks.7.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 304/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 305/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 306/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 307/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 308/1680] input_blocks.7.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 309/1680] input_blocks.7.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 310/1680] input_blocks.7.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 311/1680] input_blocks.7.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 312/1680] input_blocks.7.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 313/1680] input_blocks.7.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 314/1680] input_blocks.7.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 315/1680] input_blocks.7.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 316/1680] input_blocks.7.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 317/1680] input_blocks.7.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 318/1680] input_blocks.7.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 319/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 320/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 321/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 322/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 323/1680] input_blocks.7.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 324/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 325/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 326/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 327/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 328/1680] input_blocks.7.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 329/1680] input_blocks.7.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 330/1680] input_blocks.7.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 331/1680] input_blocks.7.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 332/1680] input_blocks.7.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 333/1680] input_blocks.7.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 334/1680] input_blocks.7.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 335/1680] input_blocks.7.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 336/1680] input_blocks.7.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 337/1680] input_blocks.7.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 338/1680] input_blocks.7.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 339/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 340/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 341/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 342/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 343/1680] input_blocks.7.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 344/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 345/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 346/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 347/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 348/1680] input_blocks.7.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 349/1680] input_blocks.7.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 350/1680] input_blocks.7.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 351/1680] input_blocks.7.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 352/1680] input_blocks.7.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 353/1680] input_blocks.7.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 354/1680] input_blocks.7.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 355/1680] input_blocks.7.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 356/1680] input_blocks.7.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 357/1680] input_blocks.7.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 358/1680] input_blocks.7.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 359/1680]   input_blocks.8.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 360/1680] input_blocks.8.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 361/1680]    input_blocks.8.0.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 362/1680]  input_blocks.8.0.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 363/1680]    input_blocks.8.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 364/1680]  input_blocks.8.0.in_layers.2.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 365/1680]   input_blocks.8.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 366/1680] input_blocks.8.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 367/1680]   input_blocks.8.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 368/1680] input_blocks.8.0.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 369/1680]           input_blocks.8.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 370/1680]         input_blocks.8.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 371/1680]        input_blocks.8.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 372/1680]      input_blocks.8.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 373/1680]       input_blocks.8.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 374/1680]     input_blocks.8.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 375/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 376/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 377/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 378/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 379/1680] input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 380/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 381/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 382/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 383/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 384/1680] input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 385/1680] input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 386/1680] input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 387/1680] input_blocks.8.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 388/1680] input_blocks.8.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 389/1680] input_blocks.8.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 390/1680] input_blocks.8.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 391/1680] input_blocks.8.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 392/1680] input_blocks.8.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 393/1680] input_blocks.8.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 394/1680] input_blocks.8.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 395/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 396/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 397/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 398/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 399/1680] input_blocks.8.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 400/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 401/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 402/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 403/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 404/1680] input_blocks.8.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 405/1680] input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 406/1680] input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 407/1680] input_blocks.8.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 408/1680] input_blocks.8.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 409/1680] input_blocks.8.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 410/1680] input_blocks.8.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 411/1680] input_blocks.8.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 412/1680] input_blocks.8.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 413/1680] input_blocks.8.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 414/1680] input_blocks.8.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 415/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 416/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 417/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 418/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 419/1680] input_blocks.8.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 420/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 421/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 422/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 423/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 424/1680] input_blocks.8.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 425/1680] input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 426/1680] input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 427/1680] input_blocks.8.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 428/1680] input_blocks.8.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 429/1680] input_blocks.8.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 430/1680] input_blocks.8.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 431/1680] input_blocks.8.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 432/1680] input_blocks.8.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 433/1680] input_blocks.8.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 434/1680] input_blocks.8.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 435/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 436/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 437/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 438/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 439/1680] input_blocks.8.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 440/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 441/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 442/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 443/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 444/1680] input_blocks.8.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 445/1680] input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 446/1680] input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 447/1680] input_blocks.8.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 448/1680] input_blocks.8.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 449/1680] input_blocks.8.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 450/1680] input_blocks.8.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 451/1680] input_blocks.8.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 452/1680] input_blocks.8.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 453/1680] input_blocks.8.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 454/1680] input_blocks.8.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 455/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 456/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 457/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 458/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 459/1680] input_blocks.8.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 460/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 461/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 462/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 463/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 464/1680] input_blocks.8.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 465/1680] input_blocks.8.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 466/1680] input_blocks.8.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 467/1680] input_blocks.8.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 468/1680] input_blocks.8.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 469/1680] input_blocks.8.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 470/1680] input_blocks.8.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 471/1680] input_blocks.8.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 472/1680] input_blocks.8.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 473/1680] input_blocks.8.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 474/1680] input_blocks.8.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 475/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 476/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 477/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 478/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 479/1680] input_blocks.8.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 480/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 481/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 482/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 483/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 484/1680] input_blocks.8.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 485/1680] input_blocks.8.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 486/1680] input_blocks.8.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 487/1680] input_blocks.8.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 488/1680] input_blocks.8.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 489/1680] input_blocks.8.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 490/1680] input_blocks.8.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 491/1680] input_blocks.8.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 492/1680] input_blocks.8.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 493/1680] input_blocks.8.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 494/1680] input_blocks.8.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 495/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 496/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 497/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 498/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 499/1680] input_blocks.8.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 500/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 501/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 502/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 503/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 504/1680] input_blocks.8.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 505/1680] input_blocks.8.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 506/1680] input_blocks.8.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 507/1680] input_blocks.8.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 508/1680] input_blocks.8.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 509/1680] input_blocks.8.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 510/1680] input_blocks.8.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 511/1680] input_blocks.8.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 512/1680] input_blocks.8.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 513/1680] input_blocks.8.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 514/1680] input_blocks.8.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 515/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 516/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 517/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 518/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 519/1680] input_blocks.8.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 520/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 521/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 522/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 523/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 524/1680] input_blocks.8.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 525/1680] input_blocks.8.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 526/1680] input_blocks.8.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 527/1680] input_blocks.8.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 528/1680] input_blocks.8.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 529/1680] input_blocks.8.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 530/1680] input_blocks.8.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 531/1680] input_blocks.8.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 532/1680] input_blocks.8.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 533/1680] input_blocks.8.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 534/1680] input_blocks.8.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 535/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 536/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 537/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 538/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 539/1680] input_blocks.8.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 540/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 541/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 542/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 543/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 544/1680] input_blocks.8.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 545/1680] input_blocks.8.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 546/1680] input_blocks.8.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 547/1680] input_blocks.8.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 548/1680] input_blocks.8.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 549/1680] input_blocks.8.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 550/1680] input_blocks.8.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 551/1680] input_blocks.8.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 552/1680] input_blocks.8.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 553/1680] input_blocks.8.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 554/1680] input_blocks.8.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 555/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 556/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 557/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 558/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 559/1680] input_blocks.8.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 560/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 561/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 562/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 563/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 564/1680] input_blocks.8.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 565/1680] input_blocks.8.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 566/1680] input_blocks.8.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 567/1680] input_blocks.8.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 568/1680] input_blocks.8.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 569/1680] input_blocks.8.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 570/1680] input_blocks.8.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 571/1680] input_blocks.8.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 572/1680] input_blocks.8.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 573/1680] input_blocks.8.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 574/1680] input_blocks.8.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 575/1680]                   label_emb.0.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 576/1680]                 label_emb.0.0.weight - [ 2816,  1280,     1,     1], type =    f16, size =    6.875 MB\n",
            "[ 577/1680]                   label_emb.0.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 578/1680]                 label_emb.0.2.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 579/1680]     middle_block.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 580/1680]   middle_block.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 581/1680]      middle_block.0.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 582/1680]    middle_block.0.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 583/1680]      middle_block.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 584/1680]    middle_block.0.in_layers.2.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 585/1680]     middle_block.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 586/1680]   middle_block.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 587/1680]     middle_block.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 588/1680]   middle_block.0.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 589/1680]             middle_block.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 590/1680]           middle_block.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 591/1680]          middle_block.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 592/1680]        middle_block.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 593/1680]         middle_block.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 594/1680]       middle_block.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 595/1680] middle_block.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 596/1680] middle_block.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 597/1680] middle_block.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 598/1680] middle_block.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 599/1680] middle_block.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 600/1680] middle_block.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 601/1680] middle_block.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 602/1680] middle_block.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 603/1680] middle_block.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 604/1680] middle_block.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 605/1680] middle_block.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 606/1680] middle_block.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 607/1680] middle_block.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 608/1680] middle_block.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 609/1680] middle_block.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 610/1680] middle_block.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 611/1680] middle_block.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 612/1680] middle_block.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 613/1680] middle_block.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 614/1680] middle_block.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 615/1680] middle_block.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 616/1680] middle_block.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 617/1680] middle_block.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 618/1680] middle_block.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 619/1680] middle_block.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 620/1680] middle_block.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 621/1680] middle_block.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 622/1680] middle_block.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 623/1680] middle_block.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 624/1680] middle_block.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 625/1680] middle_block.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 626/1680] middle_block.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 627/1680] middle_block.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 628/1680] middle_block.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 629/1680] middle_block.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 630/1680] middle_block.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 631/1680] middle_block.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 632/1680] middle_block.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 633/1680] middle_block.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 634/1680] middle_block.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 635/1680] middle_block.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 636/1680] middle_block.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 637/1680] middle_block.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 638/1680] middle_block.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 639/1680] middle_block.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 640/1680] middle_block.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 641/1680] middle_block.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 642/1680] middle_block.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 643/1680] middle_block.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 644/1680] middle_block.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 645/1680] middle_block.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 646/1680] middle_block.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 647/1680] middle_block.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 648/1680] middle_block.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 649/1680] middle_block.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 650/1680] middle_block.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 651/1680] middle_block.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 652/1680] middle_block.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 653/1680] middle_block.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 654/1680] middle_block.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 655/1680] middle_block.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 656/1680] middle_block.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 657/1680] middle_block.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 658/1680] middle_block.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 659/1680] middle_block.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 660/1680] middle_block.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 661/1680] middle_block.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 662/1680] middle_block.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 663/1680] middle_block.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 664/1680] middle_block.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 665/1680] middle_block.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 666/1680] middle_block.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 667/1680] middle_block.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 668/1680] middle_block.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 669/1680] middle_block.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 670/1680] middle_block.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 671/1680] middle_block.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 672/1680] middle_block.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 673/1680] middle_block.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 674/1680] middle_block.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 675/1680] middle_block.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 676/1680] middle_block.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 677/1680] middle_block.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 678/1680] middle_block.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 679/1680] middle_block.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 680/1680] middle_block.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 681/1680] middle_block.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 682/1680] middle_block.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 683/1680] middle_block.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 684/1680] middle_block.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 685/1680] middle_block.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 686/1680] middle_block.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 687/1680] middle_block.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 688/1680] middle_block.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 689/1680] middle_block.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 690/1680] middle_block.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 691/1680] middle_block.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 692/1680] middle_block.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 693/1680] middle_block.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 694/1680] middle_block.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 695/1680] middle_block.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 696/1680] middle_block.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 697/1680] middle_block.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 698/1680] middle_block.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 699/1680] middle_block.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 700/1680] middle_block.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 701/1680] middle_block.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 702/1680] middle_block.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 703/1680] middle_block.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 704/1680] middle_block.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 705/1680] middle_block.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 706/1680] middle_block.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 707/1680] middle_block.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 708/1680] middle_block.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 709/1680] middle_block.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 710/1680] middle_block.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 711/1680] middle_block.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 712/1680] middle_block.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 713/1680] middle_block.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 714/1680] middle_block.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 715/1680] middle_block.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 716/1680] middle_block.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 717/1680] middle_block.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 718/1680] middle_block.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 719/1680] middle_block.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 720/1680] middle_block.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 721/1680] middle_block.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 722/1680] middle_block.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 723/1680] middle_block.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 724/1680] middle_block.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 725/1680] middle_block.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 726/1680] middle_block.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 727/1680] middle_block.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 728/1680] middle_block.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 729/1680] middle_block.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 730/1680] middle_block.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 731/1680] middle_block.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 732/1680] middle_block.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 733/1680] middle_block.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 734/1680] middle_block.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 735/1680] middle_block.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 736/1680] middle_block.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 737/1680] middle_block.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 738/1680] middle_block.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 739/1680] middle_block.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 740/1680] middle_block.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 741/1680] middle_block.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 742/1680] middle_block.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 743/1680] middle_block.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 744/1680] middle_block.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 745/1680] middle_block.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 746/1680] middle_block.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 747/1680] middle_block.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 748/1680] middle_block.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 749/1680] middle_block.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 750/1680] middle_block.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 751/1680] middle_block.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 752/1680] middle_block.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 753/1680] middle_block.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 754/1680] middle_block.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 755/1680] middle_block.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 756/1680] middle_block.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 757/1680] middle_block.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 758/1680] middle_block.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 759/1680] middle_block.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 760/1680] middle_block.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 761/1680] middle_block.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 762/1680] middle_block.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 763/1680] middle_block.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 764/1680] middle_block.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 765/1680] middle_block.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 766/1680] middle_block.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 767/1680] middle_block.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 768/1680] middle_block.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 769/1680] middle_block.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 770/1680] middle_block.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 771/1680] middle_block.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 772/1680] middle_block.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 773/1680] middle_block.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 774/1680] middle_block.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 775/1680] middle_block.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 776/1680] middle_block.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 777/1680] middle_block.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 778/1680] middle_block.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 779/1680] middle_block.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 780/1680] middle_block.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 781/1680] middle_block.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 782/1680] middle_block.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 783/1680] middle_block.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 784/1680] middle_block.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 785/1680] middle_block.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 786/1680] middle_block.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 787/1680] middle_block.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 788/1680] middle_block.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 789/1680] middle_block.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 790/1680] middle_block.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 791/1680] middle_block.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 792/1680] middle_block.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 793/1680] middle_block.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 794/1680] middle_block.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 795/1680]     middle_block.2.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 796/1680]   middle_block.2.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 797/1680]      middle_block.2.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 798/1680]    middle_block.2.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 799/1680]      middle_block.2.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 800/1680]    middle_block.2.in_layers.2.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 801/1680]     middle_block.2.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 802/1680]   middle_block.2.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 803/1680]     middle_block.2.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 804/1680]   middle_block.2.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 805/1680]                           out.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 806/1680]                         out.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[ 807/1680]                           out.2.bias - [    4,     1,     1,     1], type =    f16, size =    0.000 MB\n",
            "[ 808/1680]                         out.2.weight - [  256,    45,     1,     1], type =    f16, converting to f32 .. size =     0.02 MiB ->     0.04 MiB\n",
            "[ 809/1680]  output_blocks.0.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 810/1680] output_blocks.0.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 811/1680]   output_blocks.0.0.in_layers.0.bias - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB\n",
            "[ 812/1680] output_blocks.0.0.in_layers.0.weight - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB\n",
            "[ 813/1680]   output_blocks.0.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 814/1680] output_blocks.0.0.in_layers.2.weight - [  256, 115200,     1,     1], type =    f16, converting to q5_K .. size =    56.25 MiB ->    19.34 MiB\n",
            "[ 815/1680]  output_blocks.0.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 816/1680] output_blocks.0.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 817/1680]  output_blocks.0.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 818/1680] output_blocks.0.0.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[ 819/1680] output_blocks.0.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 820/1680] output_blocks.0.0.skip_connection.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[ 821/1680]          output_blocks.0.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 822/1680]        output_blocks.0.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 823/1680]       output_blocks.0.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 824/1680]     output_blocks.0.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 825/1680]      output_blocks.0.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 826/1680]    output_blocks.0.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[ 827/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 828/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 829/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 830/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 831/1680] output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 832/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 833/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 834/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 835/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 836/1680] output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 837/1680] output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 838/1680] output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 839/1680] output_blocks.0.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 840/1680] output_blocks.0.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 841/1680] output_blocks.0.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 842/1680] output_blocks.0.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 843/1680] output_blocks.0.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 844/1680] output_blocks.0.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 845/1680] output_blocks.0.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 846/1680] output_blocks.0.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 847/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 848/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 849/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 850/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 851/1680] output_blocks.0.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 852/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 853/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 854/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 855/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 856/1680] output_blocks.0.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 857/1680] output_blocks.0.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 858/1680] output_blocks.0.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 859/1680] output_blocks.0.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 860/1680] output_blocks.0.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 861/1680] output_blocks.0.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 862/1680] output_blocks.0.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 863/1680] output_blocks.0.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 864/1680] output_blocks.0.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 865/1680] output_blocks.0.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 866/1680] output_blocks.0.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 867/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 868/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 869/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 870/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 871/1680] output_blocks.0.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 872/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 873/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 874/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 875/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 876/1680] output_blocks.0.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 877/1680] output_blocks.0.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 878/1680] output_blocks.0.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 879/1680] output_blocks.0.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 880/1680] output_blocks.0.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 881/1680] output_blocks.0.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 882/1680] output_blocks.0.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 883/1680] output_blocks.0.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 884/1680] output_blocks.0.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 885/1680] output_blocks.0.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 886/1680] output_blocks.0.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 887/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 888/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 889/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 890/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 891/1680] output_blocks.0.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 892/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 893/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 894/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 895/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 896/1680] output_blocks.0.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 897/1680] output_blocks.0.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 898/1680] output_blocks.0.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 899/1680] output_blocks.0.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 900/1680] output_blocks.0.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 901/1680] output_blocks.0.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 902/1680] output_blocks.0.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 903/1680] output_blocks.0.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 904/1680] output_blocks.0.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 905/1680] output_blocks.0.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 906/1680] output_blocks.0.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 907/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 908/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 909/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 910/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 911/1680] output_blocks.0.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 912/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 913/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 914/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 915/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 916/1680] output_blocks.0.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 917/1680] output_blocks.0.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 918/1680] output_blocks.0.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 919/1680] output_blocks.0.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 920/1680] output_blocks.0.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 921/1680] output_blocks.0.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 922/1680] output_blocks.0.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 923/1680] output_blocks.0.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 924/1680] output_blocks.0.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 925/1680] output_blocks.0.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 926/1680] output_blocks.0.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 927/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 928/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 929/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 930/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 931/1680] output_blocks.0.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 932/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 933/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 934/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 935/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 936/1680] output_blocks.0.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 937/1680] output_blocks.0.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 938/1680] output_blocks.0.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 939/1680] output_blocks.0.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 940/1680] output_blocks.0.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 941/1680] output_blocks.0.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 942/1680] output_blocks.0.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 943/1680] output_blocks.0.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 944/1680] output_blocks.0.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 945/1680] output_blocks.0.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 946/1680] output_blocks.0.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 947/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 948/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 949/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 950/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 951/1680] output_blocks.0.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 952/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 953/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 954/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 955/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 956/1680] output_blocks.0.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 957/1680] output_blocks.0.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 958/1680] output_blocks.0.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 959/1680] output_blocks.0.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 960/1680] output_blocks.0.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 961/1680] output_blocks.0.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 962/1680] output_blocks.0.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 963/1680] output_blocks.0.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 964/1680] output_blocks.0.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 965/1680] output_blocks.0.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 966/1680] output_blocks.0.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 967/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 968/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 969/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 970/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 971/1680] output_blocks.0.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 972/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 973/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 974/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 975/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 976/1680] output_blocks.0.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 977/1680] output_blocks.0.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 978/1680] output_blocks.0.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 979/1680] output_blocks.0.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 980/1680] output_blocks.0.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 981/1680] output_blocks.0.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 982/1680] output_blocks.0.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 983/1680] output_blocks.0.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 984/1680] output_blocks.0.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 985/1680] output_blocks.0.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 986/1680] output_blocks.0.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 987/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 988/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 989/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 990/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 991/1680] output_blocks.0.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 992/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 993/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[ 994/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 995/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[ 996/1680] output_blocks.0.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[ 997/1680] output_blocks.0.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[ 998/1680] output_blocks.0.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[ 999/1680] output_blocks.0.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1000/1680] output_blocks.0.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1001/1680] output_blocks.0.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1002/1680] output_blocks.0.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1003/1680] output_blocks.0.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1004/1680] output_blocks.0.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1005/1680] output_blocks.0.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1006/1680] output_blocks.0.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1007/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1008/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1009/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1010/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1011/1680] output_blocks.0.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1012/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1013/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1014/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1015/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1016/1680] output_blocks.0.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1017/1680] output_blocks.0.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1018/1680] output_blocks.0.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1019/1680] output_blocks.0.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1020/1680] output_blocks.0.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1021/1680] output_blocks.0.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1022/1680] output_blocks.0.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1023/1680] output_blocks.0.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1024/1680] output_blocks.0.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1025/1680] output_blocks.0.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1026/1680] output_blocks.0.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1027/1680]  output_blocks.1.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1028/1680] output_blocks.1.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1029/1680]   output_blocks.1.0.in_layers.0.bias - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB\n",
            "[1030/1680] output_blocks.1.0.in_layers.0.weight - [ 2560,     1,     1,     1], type =    f16, size =    0.005 MB\n",
            "[1031/1680]   output_blocks.1.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1032/1680] output_blocks.1.0.in_layers.2.weight - [  256, 115200,     1,     1], type =    f16, converting to q5_K .. size =    56.25 MiB ->    19.34 MiB\n",
            "[1033/1680]  output_blocks.1.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1034/1680] output_blocks.1.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1035/1680]  output_blocks.1.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1036/1680] output_blocks.1.0.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[1037/1680] output_blocks.1.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1038/1680] output_blocks.1.0.skip_connection.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1039/1680]          output_blocks.1.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1040/1680]        output_blocks.1.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1041/1680]       output_blocks.1.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1042/1680]     output_blocks.1.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[1043/1680]      output_blocks.1.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1044/1680]    output_blocks.1.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[1045/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1046/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1047/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1048/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1049/1680] output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1050/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1051/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1052/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1053/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1054/1680] output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1055/1680] output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1056/1680] output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1057/1680] output_blocks.1.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1058/1680] output_blocks.1.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1059/1680] output_blocks.1.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1060/1680] output_blocks.1.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1061/1680] output_blocks.1.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1062/1680] output_blocks.1.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1063/1680] output_blocks.1.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1064/1680] output_blocks.1.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1065/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1066/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1067/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1068/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1069/1680] output_blocks.1.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1070/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1071/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1072/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1073/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1074/1680] output_blocks.1.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1075/1680] output_blocks.1.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1076/1680] output_blocks.1.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1077/1680] output_blocks.1.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1078/1680] output_blocks.1.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1079/1680] output_blocks.1.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1080/1680] output_blocks.1.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1081/1680] output_blocks.1.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1082/1680] output_blocks.1.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1083/1680] output_blocks.1.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1084/1680] output_blocks.1.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1085/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1086/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1087/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1088/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1089/1680] output_blocks.1.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1090/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1091/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1092/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1093/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1094/1680] output_blocks.1.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1095/1680] output_blocks.1.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1096/1680] output_blocks.1.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1097/1680] output_blocks.1.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1098/1680] output_blocks.1.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1099/1680] output_blocks.1.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1100/1680] output_blocks.1.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1101/1680] output_blocks.1.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1102/1680] output_blocks.1.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1103/1680] output_blocks.1.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1104/1680] output_blocks.1.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1105/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1106/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1107/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1108/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1109/1680] output_blocks.1.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1110/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1111/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1112/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1113/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1114/1680] output_blocks.1.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1115/1680] output_blocks.1.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1116/1680] output_blocks.1.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1117/1680] output_blocks.1.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1118/1680] output_blocks.1.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1119/1680] output_blocks.1.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1120/1680] output_blocks.1.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1121/1680] output_blocks.1.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1122/1680] output_blocks.1.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1123/1680] output_blocks.1.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1124/1680] output_blocks.1.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1125/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1126/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1127/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1128/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1129/1680] output_blocks.1.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1130/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1131/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1132/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1133/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1134/1680] output_blocks.1.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1135/1680] output_blocks.1.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1136/1680] output_blocks.1.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1137/1680] output_blocks.1.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1138/1680] output_blocks.1.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1139/1680] output_blocks.1.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1140/1680] output_blocks.1.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1141/1680] output_blocks.1.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1142/1680] output_blocks.1.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1143/1680] output_blocks.1.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1144/1680] output_blocks.1.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1145/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1146/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1147/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1148/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1149/1680] output_blocks.1.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1150/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1151/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1152/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1153/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1154/1680] output_blocks.1.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1155/1680] output_blocks.1.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1156/1680] output_blocks.1.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1157/1680] output_blocks.1.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1158/1680] output_blocks.1.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1159/1680] output_blocks.1.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1160/1680] output_blocks.1.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1161/1680] output_blocks.1.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1162/1680] output_blocks.1.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1163/1680] output_blocks.1.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1164/1680] output_blocks.1.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1165/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1166/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1167/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1168/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1169/1680] output_blocks.1.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1170/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1171/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1172/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1173/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1174/1680] output_blocks.1.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1175/1680] output_blocks.1.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1176/1680] output_blocks.1.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1177/1680] output_blocks.1.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1178/1680] output_blocks.1.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1179/1680] output_blocks.1.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1180/1680] output_blocks.1.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1181/1680] output_blocks.1.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1182/1680] output_blocks.1.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1183/1680] output_blocks.1.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1184/1680] output_blocks.1.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1185/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1186/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1187/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1188/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1189/1680] output_blocks.1.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1190/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1191/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1192/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1193/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1194/1680] output_blocks.1.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1195/1680] output_blocks.1.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1196/1680] output_blocks.1.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1197/1680] output_blocks.1.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1198/1680] output_blocks.1.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1199/1680] output_blocks.1.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1200/1680] output_blocks.1.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1201/1680] output_blocks.1.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1202/1680] output_blocks.1.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1203/1680] output_blocks.1.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1204/1680] output_blocks.1.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1205/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1206/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1207/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1208/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1209/1680] output_blocks.1.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1210/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1211/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1212/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1213/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1214/1680] output_blocks.1.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1215/1680] output_blocks.1.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1216/1680] output_blocks.1.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1217/1680] output_blocks.1.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1218/1680] output_blocks.1.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1219/1680] output_blocks.1.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1220/1680] output_blocks.1.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1221/1680] output_blocks.1.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1222/1680] output_blocks.1.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1223/1680] output_blocks.1.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1224/1680] output_blocks.1.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1225/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1226/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1227/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1228/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1229/1680] output_blocks.1.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1230/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1231/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1232/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1233/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1234/1680] output_blocks.1.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1235/1680] output_blocks.1.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1236/1680] output_blocks.1.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1237/1680] output_blocks.1.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1238/1680] output_blocks.1.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1239/1680] output_blocks.1.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1240/1680] output_blocks.1.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1241/1680] output_blocks.1.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1242/1680] output_blocks.1.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1243/1680] output_blocks.1.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1244/1680] output_blocks.1.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1245/1680]  output_blocks.2.0.emb_layers.1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1246/1680] output_blocks.2.0.emb_layers.1.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1247/1680]   output_blocks.2.0.in_layers.0.bias - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB\n",
            "[1248/1680] output_blocks.2.0.in_layers.0.weight - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB\n",
            "[1249/1680]   output_blocks.2.0.in_layers.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1250/1680] output_blocks.2.0.in_layers.2.weight - [  256, 86400,     1,     1], type =    f16, converting to q5_K .. size =    42.19 MiB ->    14.50 MiB\n",
            "[1251/1680]  output_blocks.2.0.out_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1252/1680] output_blocks.2.0.out_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1253/1680]  output_blocks.2.0.out_layers.3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1254/1680] output_blocks.2.0.out_layers.3.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[1255/1680] output_blocks.2.0.skip_connection.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1256/1680] output_blocks.2.0.skip_connection.weight - [  256,  9600,     1,     1], type =    f16, converting to q5_K .. size =     4.69 MiB ->     1.61 MiB\n",
            "[1257/1680]          output_blocks.2.1.norm.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1258/1680]        output_blocks.2.1.norm.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1259/1680]       output_blocks.2.1.proj_in.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1260/1680]     output_blocks.2.1.proj_in.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[1261/1680]      output_blocks.2.1.proj_out.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1262/1680]    output_blocks.2.1.proj_out.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "[1263/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1264/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1265/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1266/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1267/1680] output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1268/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1269/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1270/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1271/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1272/1680] output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1273/1680] output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1274/1680] output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1275/1680] output_blocks.2.1.transformer_blocks.0.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1276/1680] output_blocks.2.1.transformer_blocks.0.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1277/1680] output_blocks.2.1.transformer_blocks.0.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1278/1680] output_blocks.2.1.transformer_blocks.0.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1279/1680] output_blocks.2.1.transformer_blocks.0.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1280/1680] output_blocks.2.1.transformer_blocks.0.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1281/1680] output_blocks.2.1.transformer_blocks.0.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1282/1680] output_blocks.2.1.transformer_blocks.0.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1283/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1284/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1285/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1286/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1287/1680] output_blocks.2.1.transformer_blocks.1.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1288/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1289/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1290/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1291/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1292/1680] output_blocks.2.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1293/1680] output_blocks.2.1.transformer_blocks.1.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1294/1680] output_blocks.2.1.transformer_blocks.1.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1295/1680] output_blocks.2.1.transformer_blocks.1.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1296/1680] output_blocks.2.1.transformer_blocks.1.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1297/1680] output_blocks.2.1.transformer_blocks.1.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1298/1680] output_blocks.2.1.transformer_blocks.1.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1299/1680] output_blocks.2.1.transformer_blocks.1.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1300/1680] output_blocks.2.1.transformer_blocks.1.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1301/1680] output_blocks.2.1.transformer_blocks.1.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1302/1680] output_blocks.2.1.transformer_blocks.1.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1303/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1304/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1305/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1306/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1307/1680] output_blocks.2.1.transformer_blocks.2.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1308/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1309/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1310/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1311/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1312/1680] output_blocks.2.1.transformer_blocks.2.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1313/1680] output_blocks.2.1.transformer_blocks.2.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1314/1680] output_blocks.2.1.transformer_blocks.2.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1315/1680] output_blocks.2.1.transformer_blocks.2.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1316/1680] output_blocks.2.1.transformer_blocks.2.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1317/1680] output_blocks.2.1.transformer_blocks.2.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1318/1680] output_blocks.2.1.transformer_blocks.2.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1319/1680] output_blocks.2.1.transformer_blocks.2.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1320/1680] output_blocks.2.1.transformer_blocks.2.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1321/1680] output_blocks.2.1.transformer_blocks.2.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1322/1680] output_blocks.2.1.transformer_blocks.2.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1323/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1324/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1325/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1326/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1327/1680] output_blocks.2.1.transformer_blocks.3.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1328/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1329/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1330/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1331/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1332/1680] output_blocks.2.1.transformer_blocks.3.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1333/1680] output_blocks.2.1.transformer_blocks.3.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1334/1680] output_blocks.2.1.transformer_blocks.3.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1335/1680] output_blocks.2.1.transformer_blocks.3.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1336/1680] output_blocks.2.1.transformer_blocks.3.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1337/1680] output_blocks.2.1.transformer_blocks.3.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1338/1680] output_blocks.2.1.transformer_blocks.3.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1339/1680] output_blocks.2.1.transformer_blocks.3.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1340/1680] output_blocks.2.1.transformer_blocks.3.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1341/1680] output_blocks.2.1.transformer_blocks.3.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1342/1680] output_blocks.2.1.transformer_blocks.3.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1343/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1344/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1345/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1346/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1347/1680] output_blocks.2.1.transformer_blocks.4.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1348/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1349/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1350/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1351/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1352/1680] output_blocks.2.1.transformer_blocks.4.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1353/1680] output_blocks.2.1.transformer_blocks.4.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1354/1680] output_blocks.2.1.transformer_blocks.4.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1355/1680] output_blocks.2.1.transformer_blocks.4.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1356/1680] output_blocks.2.1.transformer_blocks.4.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1357/1680] output_blocks.2.1.transformer_blocks.4.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1358/1680] output_blocks.2.1.transformer_blocks.4.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1359/1680] output_blocks.2.1.transformer_blocks.4.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1360/1680] output_blocks.2.1.transformer_blocks.4.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1361/1680] output_blocks.2.1.transformer_blocks.4.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1362/1680] output_blocks.2.1.transformer_blocks.4.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1363/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1364/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1365/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1366/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1367/1680] output_blocks.2.1.transformer_blocks.5.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1368/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1369/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1370/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1371/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1372/1680] output_blocks.2.1.transformer_blocks.5.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1373/1680] output_blocks.2.1.transformer_blocks.5.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1374/1680] output_blocks.2.1.transformer_blocks.5.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1375/1680] output_blocks.2.1.transformer_blocks.5.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1376/1680] output_blocks.2.1.transformer_blocks.5.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1377/1680] output_blocks.2.1.transformer_blocks.5.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1378/1680] output_blocks.2.1.transformer_blocks.5.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1379/1680] output_blocks.2.1.transformer_blocks.5.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1380/1680] output_blocks.2.1.transformer_blocks.5.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1381/1680] output_blocks.2.1.transformer_blocks.5.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1382/1680] output_blocks.2.1.transformer_blocks.5.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1383/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1384/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1385/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1386/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1387/1680] output_blocks.2.1.transformer_blocks.6.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1388/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1389/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1390/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1391/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1392/1680] output_blocks.2.1.transformer_blocks.6.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1393/1680] output_blocks.2.1.transformer_blocks.6.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1394/1680] output_blocks.2.1.transformer_blocks.6.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1395/1680] output_blocks.2.1.transformer_blocks.6.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1396/1680] output_blocks.2.1.transformer_blocks.6.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1397/1680] output_blocks.2.1.transformer_blocks.6.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1398/1680] output_blocks.2.1.transformer_blocks.6.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1399/1680] output_blocks.2.1.transformer_blocks.6.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1400/1680] output_blocks.2.1.transformer_blocks.6.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1401/1680] output_blocks.2.1.transformer_blocks.6.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1402/1680] output_blocks.2.1.transformer_blocks.6.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1403/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1404/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1405/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1406/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1407/1680] output_blocks.2.1.transformer_blocks.7.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1408/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1409/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1410/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1411/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1412/1680] output_blocks.2.1.transformer_blocks.7.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1413/1680] output_blocks.2.1.transformer_blocks.7.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1414/1680] output_blocks.2.1.transformer_blocks.7.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1415/1680] output_blocks.2.1.transformer_blocks.7.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1416/1680] output_blocks.2.1.transformer_blocks.7.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1417/1680] output_blocks.2.1.transformer_blocks.7.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1418/1680] output_blocks.2.1.transformer_blocks.7.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1419/1680] output_blocks.2.1.transformer_blocks.7.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1420/1680] output_blocks.2.1.transformer_blocks.7.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1421/1680] output_blocks.2.1.transformer_blocks.7.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1422/1680] output_blocks.2.1.transformer_blocks.7.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1423/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1424/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1425/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1426/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1427/1680] output_blocks.2.1.transformer_blocks.8.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1428/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1429/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1430/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1431/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1432/1680] output_blocks.2.1.transformer_blocks.8.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1433/1680] output_blocks.2.1.transformer_blocks.8.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1434/1680] output_blocks.2.1.transformer_blocks.8.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1435/1680] output_blocks.2.1.transformer_blocks.8.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1436/1680] output_blocks.2.1.transformer_blocks.8.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1437/1680] output_blocks.2.1.transformer_blocks.8.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1438/1680] output_blocks.2.1.transformer_blocks.8.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1439/1680] output_blocks.2.1.transformer_blocks.8.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1440/1680] output_blocks.2.1.transformer_blocks.8.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1441/1680] output_blocks.2.1.transformer_blocks.8.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1442/1680] output_blocks.2.1.transformer_blocks.8.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1443/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_k.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1444/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1445/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1446/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1447/1680] output_blocks.2.1.transformer_blocks.9.attn1.to_v.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1448/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_k.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1449/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_out.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1450/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_out.0.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1451/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_q.weight - [ 1280,  1280,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1452/1680] output_blocks.2.1.transformer_blocks.9.attn2.to_v.weight - [ 2048,  1280,     1,     1], type =    f16, converting to q5_K .. size =     5.00 MiB ->     1.72 MiB\n",
            "[1453/1680] output_blocks.2.1.transformer_blocks.9.ff.net.0.proj.bias - [10240,     1,     1,     1], type =    f16, size =    0.020 MB\n",
            "[1454/1680] output_blocks.2.1.transformer_blocks.9.ff.net.0.proj.weight - [ 1280, 10240,     1,     1], type =    f16, converting to q5_K .. size =    25.00 MiB ->     8.59 MiB\n",
            "[1455/1680] output_blocks.2.1.transformer_blocks.9.ff.net.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1456/1680] output_blocks.2.1.transformer_blocks.9.ff.net.2.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[1457/1680] output_blocks.2.1.transformer_blocks.9.norm1.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1458/1680] output_blocks.2.1.transformer_blocks.9.norm1.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1459/1680] output_blocks.2.1.transformer_blocks.9.norm2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1460/1680] output_blocks.2.1.transformer_blocks.9.norm2.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1461/1680] output_blocks.2.1.transformer_blocks.9.norm3.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1462/1680] output_blocks.2.1.transformer_blocks.9.norm3.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1463/1680]          output_blocks.2.2.conv.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1464/1680]        output_blocks.2.2.conv.weight - [  256, 57600,     1,     1], type =    f16, converting to q5_K .. size =    28.12 MiB ->     9.67 MiB\n",
            "[1465/1680]  output_blocks.3.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1466/1680] output_blocks.3.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[1467/1680]   output_blocks.3.0.in_layers.0.bias - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB\n",
            "[1468/1680] output_blocks.3.0.in_layers.0.weight - [ 1920,     1,     1,     1], type =    f16, size =    0.004 MB\n",
            "[1469/1680]   output_blocks.3.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1470/1680] output_blocks.3.0.in_layers.2.weight - [  256, 43200,     1,     1], type =    f16, converting to q5_K .. size =    21.09 MiB ->     7.25 MiB\n",
            "[1471/1680]  output_blocks.3.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1472/1680] output_blocks.3.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1473/1680]  output_blocks.3.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1474/1680] output_blocks.3.0.out_layers.3.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[1475/1680] output_blocks.3.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1476/1680] output_blocks.3.0.skip_connection.weight - [  256,  4800,     1,     1], type =    f16, converting to q5_K .. size =     2.34 MiB ->     0.81 MiB\n",
            "[1477/1680]          output_blocks.3.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1478/1680]        output_blocks.3.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1479/1680]       output_blocks.3.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1480/1680]     output_blocks.3.1.proj_in.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1481/1680]      output_blocks.3.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1482/1680]    output_blocks.3.1.proj_out.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1483/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1484/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1485/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1486/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1487/1680] output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1488/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1489/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1490/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1491/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1492/1680] output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1493/1680] output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[1494/1680] output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1495/1680] output_blocks.3.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1496/1680] output_blocks.3.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1497/1680] output_blocks.3.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1498/1680] output_blocks.3.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1499/1680] output_blocks.3.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1500/1680] output_blocks.3.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1501/1680] output_blocks.3.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1502/1680] output_blocks.3.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1503/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1504/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1505/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1506/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1507/1680] output_blocks.3.1.transformer_blocks.1.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1508/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1509/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1510/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1511/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1512/1680] output_blocks.3.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1513/1680] output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[1514/1680] output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1515/1680] output_blocks.3.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1516/1680] output_blocks.3.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1517/1680] output_blocks.3.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1518/1680] output_blocks.3.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1519/1680] output_blocks.3.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1520/1680] output_blocks.3.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1521/1680] output_blocks.3.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1522/1680] output_blocks.3.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1523/1680]  output_blocks.4.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1524/1680] output_blocks.4.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[1525/1680]   output_blocks.4.0.in_layers.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1526/1680] output_blocks.4.0.in_layers.0.weight - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1527/1680]   output_blocks.4.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1528/1680] output_blocks.4.0.in_layers.2.weight - [  256, 28800,     1,     1], type =    f16, converting to q5_K .. size =    14.06 MiB ->     4.83 MiB\n",
            "[1529/1680]  output_blocks.4.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1530/1680] output_blocks.4.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1531/1680]  output_blocks.4.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1532/1680] output_blocks.4.0.out_layers.3.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[1533/1680] output_blocks.4.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1534/1680] output_blocks.4.0.skip_connection.weight - [  256,  3200,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[1535/1680]          output_blocks.4.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1536/1680]        output_blocks.4.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1537/1680]       output_blocks.4.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1538/1680]     output_blocks.4.1.proj_in.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1539/1680]      output_blocks.4.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1540/1680]    output_blocks.4.1.proj_out.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1541/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1542/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1543/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1544/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1545/1680] output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1546/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1547/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1548/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1549/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1550/1680] output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1551/1680] output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[1552/1680] output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1553/1680] output_blocks.4.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1554/1680] output_blocks.4.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1555/1680] output_blocks.4.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1556/1680] output_blocks.4.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1557/1680] output_blocks.4.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1558/1680] output_blocks.4.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1559/1680] output_blocks.4.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1560/1680] output_blocks.4.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1561/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1562/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1563/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1564/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1565/1680] output_blocks.4.1.transformer_blocks.1.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1566/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1567/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1568/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1569/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1570/1680] output_blocks.4.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1571/1680] output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[1572/1680] output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1573/1680] output_blocks.4.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1574/1680] output_blocks.4.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1575/1680] output_blocks.4.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1576/1680] output_blocks.4.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1577/1680] output_blocks.4.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1578/1680] output_blocks.4.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1579/1680] output_blocks.4.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1580/1680] output_blocks.4.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1581/1680]  output_blocks.5.0.emb_layers.1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1582/1680] output_blocks.5.0.emb_layers.1.weight - [ 1280,   640,     1,     1], type =    f16, converting to q5_K .. size =     1.56 MiB ->     0.54 MiB\n",
            "[1583/1680]   output_blocks.5.0.in_layers.0.bias - [  960,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1584/1680] output_blocks.5.0.in_layers.0.weight - [  960,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1585/1680]   output_blocks.5.0.in_layers.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1586/1680] output_blocks.5.0.in_layers.2.weight - [  256, 21600,     1,     1], type =    f16, converting to q5_K .. size =    10.55 MiB ->     3.63 MiB\n",
            "[1587/1680]  output_blocks.5.0.out_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1588/1680] output_blocks.5.0.out_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1589/1680]  output_blocks.5.0.out_layers.3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1590/1680] output_blocks.5.0.out_layers.3.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[1591/1680] output_blocks.5.0.skip_connection.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1592/1680] output_blocks.5.0.skip_connection.weight - [  256,  2400,     1,     1], type =    f16, converting to q5_K .. size =     1.17 MiB ->     0.40 MiB\n",
            "[1593/1680]          output_blocks.5.1.norm.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1594/1680]        output_blocks.5.1.norm.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1595/1680]       output_blocks.5.1.proj_in.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1596/1680]     output_blocks.5.1.proj_in.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1597/1680]      output_blocks.5.1.proj_out.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1598/1680]    output_blocks.5.1.proj_out.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1599/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1600/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1601/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1602/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1603/1680] output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1604/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1605/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1606/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1607/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1608/1680] output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1609/1680] output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[1610/1680] output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1611/1680] output_blocks.5.1.transformer_blocks.0.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1612/1680] output_blocks.5.1.transformer_blocks.0.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1613/1680] output_blocks.5.1.transformer_blocks.0.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1614/1680] output_blocks.5.1.transformer_blocks.0.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1615/1680] output_blocks.5.1.transformer_blocks.0.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1616/1680] output_blocks.5.1.transformer_blocks.0.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1617/1680] output_blocks.5.1.transformer_blocks.0.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1618/1680] output_blocks.5.1.transformer_blocks.0.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1619/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_k.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1620/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1621/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1622/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1623/1680] output_blocks.5.1.transformer_blocks.1.attn1.to_v.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1624/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_k.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1625/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1626/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1627/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_q.weight - [  256,  1600,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1628/1680] output_blocks.5.1.transformer_blocks.1.attn2.to_v.weight - [ 2048,   640,     1,     1], type =    f16, converting to q5_K .. size =     2.50 MiB ->     0.86 MiB\n",
            "[1629/1680] output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.bias - [ 5120,     1,     1,     1], type =    f16, size =    0.010 MB\n",
            "[1630/1680] output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight - [  256, 12800,     1,     1], type =    f16, converting to q5_K .. size =     6.25 MiB ->     2.15 MiB\n",
            "[1631/1680] output_blocks.5.1.transformer_blocks.1.ff.net.2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1632/1680] output_blocks.5.1.transformer_blocks.1.ff.net.2.weight - [ 2560,   640,     1,     1], type =    f16, converting to q5_K .. size =     3.12 MiB ->     1.07 MiB\n",
            "[1633/1680] output_blocks.5.1.transformer_blocks.1.norm1.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1634/1680] output_blocks.5.1.transformer_blocks.1.norm1.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1635/1680] output_blocks.5.1.transformer_blocks.1.norm2.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1636/1680] output_blocks.5.1.transformer_blocks.1.norm2.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1637/1680] output_blocks.5.1.transformer_blocks.1.norm3.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1638/1680] output_blocks.5.1.transformer_blocks.1.norm3.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1639/1680]          output_blocks.5.2.conv.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1640/1680]        output_blocks.5.2.conv.weight - [  256, 14400,     1,     1], type =    f16, converting to q5_K .. size =     7.03 MiB ->     2.42 MiB\n",
            "[1641/1680]  output_blocks.6.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1642/1680] output_blocks.6.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1643/1680]   output_blocks.6.0.in_layers.0.bias - [  960,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1644/1680] output_blocks.6.0.in_layers.0.weight - [  960,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1645/1680]   output_blocks.6.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1646/1680] output_blocks.6.0.in_layers.2.weight - [  256, 10800,     1,     1], type =    f16, converting to q5_K .. size =     5.27 MiB ->     1.81 MiB\n",
            "[1647/1680]  output_blocks.6.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1648/1680] output_blocks.6.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1649/1680]  output_blocks.6.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1650/1680] output_blocks.6.0.out_layers.3.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[1651/1680] output_blocks.6.0.skip_connection.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1652/1680] output_blocks.6.0.skip_connection.weight - [  256,  1200,     1,     1], type =    f16, converting to q5_K .. size =     0.59 MiB ->     0.20 MiB\n",
            "[1653/1680]  output_blocks.7.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1654/1680] output_blocks.7.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1655/1680]   output_blocks.7.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1656/1680] output_blocks.7.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1657/1680]   output_blocks.7.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1658/1680] output_blocks.7.0.in_layers.2.weight - [  256,  7200,     1,     1], type =    f16, converting to q5_K .. size =     3.52 MiB ->     1.21 MiB\n",
            "[1659/1680]  output_blocks.7.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1660/1680] output_blocks.7.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1661/1680]  output_blocks.7.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1662/1680] output_blocks.7.0.out_layers.3.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[1663/1680] output_blocks.7.0.skip_connection.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1664/1680] output_blocks.7.0.skip_connection.weight - [  256,   800,     1,     1], type =    f16, converting to q5_K .. size =     0.39 MiB ->     0.13 MiB\n",
            "[1665/1680]  output_blocks.8.0.emb_layers.1.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1666/1680] output_blocks.8.0.emb_layers.1.weight - [ 1280,   320,     1,     1], type =    f16, converting to q5_K .. size =     0.78 MiB ->     0.27 MiB\n",
            "[1667/1680]   output_blocks.8.0.in_layers.0.bias - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1668/1680] output_blocks.8.0.in_layers.0.weight - [  640,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1669/1680]   output_blocks.8.0.in_layers.2.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1670/1680] output_blocks.8.0.in_layers.2.weight - [  256,  7200,     1,     1], type =    f16, converting to q5_K .. size =     3.52 MiB ->     1.21 MiB\n",
            "[1671/1680]  output_blocks.8.0.out_layers.0.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1672/1680] output_blocks.8.0.out_layers.0.weight - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1673/1680]  output_blocks.8.0.out_layers.3.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1674/1680] output_blocks.8.0.out_layers.3.weight - [  256,  3600,     1,     1], type =    f16, converting to q5_K .. size =     1.76 MiB ->     0.60 MiB\n",
            "[1675/1680] output_blocks.8.0.skip_connection.bias - [  320,     1,     1,     1], type =    f16, size =    0.001 MB\n",
            "[1676/1680] output_blocks.8.0.skip_connection.weight - [  256,   800,     1,     1], type =    f16, converting to q5_K .. size =     0.39 MiB ->     0.13 MiB\n",
            "[1677/1680]                    time_embed.0.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1678/1680]                  time_embed.0.weight - [  256,  1600,     1,     1], type =    f16, size =    0.781 MB\n",
            "[1679/1680]                    time_embed.2.bias - [ 1280,     1,     1,     1], type =    f16, size =    0.002 MB\n",
            "[1680/1680]                  time_embed.2.weight - [ 1280,  1280,     1,     1], type =    f16, size =    3.125 MB\n",
            "llama_model_quantize_internal: model size  =  4897.05 MB\n",
            "llama_model_quantize_internal: quant size  =  1724.28 MB\n",
            "\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m \u001b[1;36mQuantizing to Q5_K_S\u001b[0m \u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mProcessing...\u001b[0m \u001b[33m0:04:59\u001b[0m2025-03-09 16:39:30,929 - INFO - Quantization complete. Output saved to: quantized/bluepencil-xl_Q5_K_S.gguf\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m \u001b[1;36mQuantizing to Q5_K_S\u001b[0m \u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[35m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[90m━\u001b[0m\u001b[35m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m\u001b[91m━\u001b[0m \u001b[1;33mComplete!\u001b[0m \u001b[33m0:04:59\u001b[0m\n",
            "\u001b[?25h\u001b[32m✓ Quantization to Q5_K_S complete: \u001b[0m\u001b[1;32mquantized/bluepencil-xl_Q5_K_S.gguf\u001b[0m\n",
            "\u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────────────\u001b[0m\u001b[33m \u001b[0m\u001b[1;33mQuantization Results\u001b[0m\u001b[33m \u001b[0m\u001b[33m─────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
            "\u001b[33m│\u001b[0m ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m ┃\u001b[1;35m \u001b[0m\u001b[1;35mModel Type    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFile Size \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m% of Original\u001b[0m\u001b[1;35m \u001b[0m┃                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m │\u001b[36m \u001b[0m\u001b[36mOriginal Model\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m4897.20 MB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m100%         \u001b[0m\u001b[33m \u001b[0m│                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m │\u001b[36m \u001b[0m\u001b[36mS             \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1724.43 MB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m35.2%        \u001b[0m\u001b[33m \u001b[0m│                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m│\u001b[0m └────────────────┴────────────┴───────────────┘                                                  \u001b[33m│\u001b[0m\n",
            "\u001b[33m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[32m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
            "\u001b[32m│\u001b[0m \u001b[1;32mProcessing pipeline complete!\u001b[0m                                                                    \u001b[32m│\u001b[0m\n",
            "\u001b[32m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python new-gguf-test/main.py --civitai \\\n",
        "--model_name \"bluepencil-xl\" \\\n",
        "--model_version_id 592322 \\\n",
        "--civitai_token \"add_civit_ai_token\" \\\n",
        "--download_dir \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeOJEQkhmk4K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
